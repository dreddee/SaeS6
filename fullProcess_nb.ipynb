{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generation_folder = \"generated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction d'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_simple(data,model=None):\n",
    "    size_vocabulary = 1000\n",
    "    X = None\n",
    "    if model :\n",
    "        X = model.transform(data)\n",
    "    else:\n",
    "        model = CountVectorizer(stop_words = \"english\", max_features = size_vocabulary,ngram_range=(1, 2))\n",
    "        X = model.fit_transform(data)\n",
    "    return X.toarray(),model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_tf_idf(data,model=None):\n",
    "    size_vocabulary = 1000\n",
    "    X = None\n",
    "    if model :\n",
    "        X = model.transform(data)\n",
    "    else:\n",
    "        model = TfidfVectorizer(stop_words = \"english\", max_features = size_vocabulary,ngram_range=(1, 2))\n",
    "        X = model.fit_transform(data)\n",
    "    return X.toarray(),model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_latent_representation(textList,tokenizer,model):\n",
    "    inputs = tokenizer(textList, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return cls_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_dict = {\n",
    "    \"simple\": retrieve_simple,\n",
    "    \"tf_idf\": retrieve_tf_idf,\n",
    "    \"bert\": lambda data, model : get_bert_latent_representation(data.values.tolist(),tokenizer,model)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction de model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"RandomForest\": (lambda : RandomForestClassifier(n_estimators=100)),\n",
    "    \"GradienBoost\": (lambda : GradientBoostingClassifier(n_estimators=100))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model(X,Y,model=None):\n",
    "    if model in model_dict:\n",
    "        model = model_dict[model]()\n",
    "    \n",
    "    model.fit(X, Y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_matrix_column(df, column_name):\n",
    "    # Convertir la liste de listes en array numpy\n",
    "    matrix = np.array(df[column_name].tolist())\n",
    "    # Créer des noms de colonnes\n",
    "    column_names = [f\"{column_name}_{i}\" for i in range(matrix.shape[1])]\n",
    "    # Retourner un nouveau DataFrame avec les colonnes expandées\n",
    "    return pd.DataFrame(matrix, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchDataframe(df, Batch_size):\n",
    "    \"\"\"\n",
    "    Renvoie une liste de sous-DataFrames en batchs de taille Batch_size, après mélange des données.\n",
    "\n",
    "    Paramètres :\n",
    "    df : pandas.DataFrame\n",
    "        Le DataFrame à diviser en lots.\n",
    "    Batch_size : int\n",
    "        La taille de chaque lot.\n",
    "\n",
    "    Retour :\n",
    "    list of pandas.DataFrame\n",
    "        Une liste de DataFrames contenant les lots mélangés.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Diviser en batchs\n",
    "    batches = [df.iloc[i:i + Batch_size] for i in range(0, len(df), Batch_size)]\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcess(data,embedding,summary_embedded_model,comment_embedded_model):\n",
    "    summary_matrix, summary_embedded_model = embedded_dict[embedding](data['summary'],summary_embedded_model)\n",
    "    comment_matrix, comment_embedded_model = embedded_dict[embedding](data['comment'],comment_embedded_model)\n",
    "    \n",
    "    data['summary'] = list(summary_matrix)\n",
    "    data['comment'] = list(comment_matrix)\n",
    "\n",
    "    data = data.drop(columns=['Titre']) # A faire plus tot\n",
    "\n",
    "    X_simple = data.drop(columns=['rating'])\n",
    "    Y_simple = data['rating'] \n",
    "\n",
    "    X_add = pd.concat([\n",
    "                    expand_matrix_column(data, 'summary'),\n",
    "                    expand_matrix_column(data, 'comment')\n",
    "                ], axis=1)\n",
    "    \n",
    "    X_simple.drop(columns=['summary', 'comment'])\n",
    "\n",
    "    X_expanded = pd.concat([\n",
    "                    X_simple,\n",
    "                    X_add\n",
    "                ], axis=1)\n",
    "    return X_add, Y_simple, summary_embedded_model, comment_embedded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchProcess(data,embedding=\"simple\",model=\"RandomForest\",BatchSize=3000):\n",
    "    data_train, data_test = train_test_split(data, test_size=0.2)\n",
    "\n",
    "    batch_train = BatchDataframe(data_train,BatchSize)\n",
    "\n",
    "    summary_embedded_model = None\n",
    "    comment_embedded_model = None\n",
    "    if(embedding==\"bert\"):\n",
    "        summary_embedded_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        comment_embedded_model = summary_embedded_model\n",
    "    model_chosen = model\n",
    "\n",
    "    print(\"Training the model:\")\n",
    "    for true_batch in tqdm(batch_train, desc=\"Training Progress\"):\n",
    "        batch = true_batch.copy()\n",
    "\n",
    "        X_expanded,Y_simple,summary_embedded_model,comment_embedded_model = PreProcess(batch,embedding,summary_embedded_model,comment_embedded_model)\n",
    "        \n",
    "        model_chosen = process_model(X_expanded,Y_simple,model_chosen)\n",
    "    \n",
    "    accuracy_process = np.array([])\n",
    "\n",
    "\n",
    "    batch_test = BatchDataframe(data_test,BatchSize)\n",
    "    print(\"\\nEvaluating the model:\")\n",
    "    for true_batch in tqdm(batch_test, desc=\"Evaluation Progress\"):\n",
    "        batch = true_batch.copy()\n",
    "\n",
    "        X_expanded,Y_simple,summary_embedded_model,comment_embedded_model = PreProcess(batch,embedding,summary_embedded_model,comment_embedded_model)\n",
    "\n",
    "        y_pred = model_chosen.predict(X_expanded)\n",
    "        accuracy = accuracy_score(Y_simple, y_pred)\n",
    "        accuracy_process = np.append(accuracy_process,accuracy)\n",
    "\n",
    "    print(f'Model Accuracy: {np.mean(accuracy_process):.2f}')\n",
    "    return model_chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(os.path.join(Generation_folder, 'ratings_formatted.csv')).dropna(subset=['summary']).dropna(subset=['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "BatchProcess(ratings.head(20000),embedding=\"bert\",BatchSize=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
