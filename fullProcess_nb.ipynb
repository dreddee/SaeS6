{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ficher de process general permettant la gestion de batch ainsi que le test de differentes combinaison d'embedding et de models\n",
    "- Batch permet d'executer les gros datasets permettant a tout le monde a l'executer (et surement le prof si necessaire)\n",
    "- Facilitte la vision des fonctions inutile (Et ainsi eviter de perdre des points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generation_folder = \"generated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction d'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_simple(data,model=None):\n",
    "    size_vocabulary = 1000\n",
    "    X = None\n",
    "    if model :\n",
    "        X = model.transform(data)\n",
    "    else:\n",
    "        model = CountVectorizer(stop_words = \"english\", max_features = size_vocabulary,ngram_range=(1, 2))\n",
    "        X = model.fit_transform(data)\n",
    "    return X.toarray(),model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_tf_idf(data,model=None):\n",
    "    size_vocabulary = 1000\n",
    "    X = None\n",
    "    if model :\n",
    "        X = model.transform(data)\n",
    "    else:\n",
    "        model = TfidfVectorizer(stop_words = \"english\", max_features = size_vocabulary,ngram_range=(1, 2))\n",
    "        X = model.fit_transform(data)\n",
    "    return X.toarray(),model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Berttokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "GPTtokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "GPTtokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_representation(textList,tokenizer,model):\n",
    "    inputs = tokenizer(textList, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return cls_embedding, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_dict = {\n",
    "    \"simple\": retrieve_simple,\n",
    "    \"tf_idf\": retrieve_tf_idf,\n",
    "    \"bert\": lambda data, model : get_latent_representation(data.values.tolist(),Berttokenizer,model),\n",
    "    \"gpt2\": lambda data, model : get_latent_representation(data.values.tolist(),GPTtokenizer,model)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction de model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Deepmodel():\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(5, activation='softmax')  # 5 classes => 5 neurones en sortie\n",
    "    ])\n",
    "    \n",
    "    # Compilation du modèle (labels sous forme d'entiers 1-5)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',  # Adapté aux labels entiers\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    training_arg = { \n",
    "        \"epochs\":10,\n",
    "        \"batch_size\":32,\n",
    "        \"verbose\":0\n",
    "    }\n",
    "\n",
    "    return model, training_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"RandomForest\": (lambda : (RandomForestClassifier(n_estimators=100), {})),\n",
    "    \"GradienBoost\": (lambda : (GradientBoostingClassifier(n_estimators=100), {})),\n",
    "    \"knn\": (lambda :( KNeighborsClassifier(n_neighbors=5), {})),\n",
    "    \"naive\": (lambda : (MultinomialNB(), {})),\n",
    "    \"logistic\": (lambda : (LogisticRegression(max_iter=1000, multi_class='multinomial'), {})),\n",
    "    \"decisiontree\": (lambda : DecisionTreeClassifier(), {}),\n",
    "    \"mlpClass\": (lambda : (MLPClassifier(hidden_layer_sizes=(100,), max_iter=500), {})),\n",
    "    \"deep\": Deepmodel\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model(X,Y,model=None,arg={}):\n",
    "    if model in model_dict:\n",
    "        model, arg = model_dict[model]()\n",
    "    \n",
    "    model.fit(X, Y,**arg)\n",
    "    return model, arg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_transform_dict = {\n",
    "    \"deep\": (lambda data: np.argmax(data, axis=1) + 1 ), # argmax renvoit l'indice de la valeur la plus haute du tableau\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_matrix_column(df, column_name):\n",
    "    # Convertir la liste de listes en array numpy\n",
    "    matrix = np.array(df[column_name].tolist())\n",
    "    # Créer des noms de colonnes\n",
    "    column_names = [f\"{column_name}_{i}\" for i in range(matrix.shape[1])]\n",
    "    # Retourner un nouveau DataFrame avec les colonnes expandées\n",
    "    return pd.DataFrame(matrix, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchDataframe(df, Batch_size):\n",
    "    \"\"\"\n",
    "    Renvoie une liste de sous-DataFrames en batchs de taille Batch_size, après mélange des données.\n",
    "\n",
    "    Paramètres :\n",
    "    df : pandas.DataFrame\n",
    "        Le DataFrame à diviser en lots.\n",
    "    Batch_size : int\n",
    "        La taille de chaque lot.\n",
    "\n",
    "    Retour :\n",
    "    list of pandas.DataFrame\n",
    "        Une liste de DataFrames contenant les lots mélangés.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Diviser en batchs\n",
    "    batches = [df.iloc[i:i + Batch_size] for i in range(0, len(df), Batch_size)]\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcess(data,embedding,summary_embedded_model,comment_embedded_model):\n",
    "    summary_matrix, summary_embedded_model = embedded_dict[embedding](data['summary'],summary_embedded_model)\n",
    "    comment_matrix, comment_embedded_model = embedded_dict[embedding](data['comment'],comment_embedded_model)\n",
    "    \n",
    "    data['summary'] = list(summary_matrix)\n",
    "    data['comment'] = list(comment_matrix)\n",
    "\n",
    "    data = data.drop(columns=['Titre']) # A faire plus tot\n",
    "\n",
    "    X_simple = data.drop(columns=['rating'])\n",
    "    Y_simple = data['rating'] \n",
    "\n",
    "    X_add = pd.concat([\n",
    "                    expand_matrix_column(data, 'summary'),\n",
    "                    expand_matrix_column(data, 'comment')\n",
    "                ], axis=1)\n",
    "    \n",
    "    X_simple.drop(columns=['summary', 'comment'])\n",
    "\n",
    "    X_expanded = pd.concat([\n",
    "                    X_simple,\n",
    "                    X_add\n",
    "                ], axis=1)\n",
    "    return X_add, Y_simple, summary_embedded_model, comment_embedded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchProcess(data,embedding=\"simple\",model=\"RandomForest\",BatchSize=3000,enable_batch=True):\n",
    "    data_train, data_test = train_test_split(data, test_size=0.2)\n",
    "\n",
    "    training_arg = {}\n",
    "\n",
    "    if enable_batch:\n",
    "        batch_train = BatchDataframe(data_train,BatchSize)\n",
    "    else:\n",
    "        batch_train = [data_train]\n",
    "\n",
    "    summary_embedded_model = None\n",
    "    comment_embedded_model = None\n",
    "    if(embedding==\"bert\"):\n",
    "        summary_embedded_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        comment_embedded_model = summary_embedded_model\n",
    "    elif(embedding==\"gpt2\"):\n",
    "        summary_embedded_model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "        comment_embedded_model = summary_embedded_model\n",
    "    model_chosen = model\n",
    "\n",
    "    print(\"Training the model:\")\n",
    "    for true_batch in tqdm(batch_train, desc=\"Training Progress\"):\n",
    "        batch = true_batch.copy()\n",
    "\n",
    "        X_expanded,Y_simple,summary_embedded_model,comment_embedded_model = PreProcess(batch,embedding,summary_embedded_model,comment_embedded_model)\n",
    "        \n",
    "        model_chosen, training_arg = process_model(X_expanded,Y_simple,model_chosen, training_arg)\n",
    "    \n",
    "    accuracy_process = np.array([])\n",
    "    \n",
    "    if enable_batch:\n",
    "        batch_test = BatchDataframe(data_test,BatchSize)\n",
    "    else:\n",
    "        batch_test = [data_test]\n",
    "\n",
    "    print(\"\\nEvaluating the model:\")\n",
    "    for true_batch in tqdm(batch_test, desc=\"Evaluation Progress\"):\n",
    "        batch = true_batch.copy()\n",
    "\n",
    "        X_expanded,Y_simple,summary_embedded_model,comment_embedded_model = PreProcess(batch,embedding,summary_embedded_model,comment_embedded_model)\n",
    "\n",
    "        y_pred = model_chosen.predict(X_expanded)\n",
    "        \n",
    "        if model in pred_transform_dict:\n",
    "            y_pred = pred_transform_dict[model](y_pred)\n",
    "\n",
    "        accuracy = accuracy_score(Y_simple, y_pred)\n",
    "        accuracy_process = np.append(accuracy_process,accuracy)\n",
    "\n",
    "    print(f'Model Accuracy: {np.mean(accuracy_process):.2f}')\n",
    "    return model_chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(os.path.join(Generation_folder, 'ratings_formatted.csv')).dropna(subset=['summary']).dropna(subset=['comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple\n",
      "tf_idf\n",
      "bert\n",
      "gpt2\n"
     ]
    }
   ],
   "source": [
    "print(*list(embedded_dict.keys()),sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest\n",
      "GradienBoost\n",
      "knn\n",
      "naive\n",
      "logistic\n",
      "decisiontree\n",
      "mlpClass\n",
      "deep\n"
     ]
    }
   ],
   "source": [
    "print(*list(model_dict.keys()),sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8/8 [00:12<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Progress:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Progress:  50%|█████     | 1/2 [00:00<00:00,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Progress: 100%|██████████| 2/2 [00:00<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "modelProcess = BatchProcess(ratings.head(10000),embedding=\"tf_idf\",model=\"deep\",BatchSize=1000,enable_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du modèle\n",
    "with open(os.path.join(Generation_folder,\"model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(modelProcess, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modèle\n",
    "with open(os.path.join(Generation_folder,\"model.pkl\"), \"rb\") as f:\n",
    "    modelPickle = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.sequential.Sequential at 0x214991dc0d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelPickle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
